import asyncio
import requests
from bs4 import BeautifulSoup
import re
import os
import requests
import json
import uvicorn
import logging
import vk_api
from datetime import datetime
from typing import List, Optional
from fastapi import Query
from fastapi.responses import StreamingResponse
import csv
import io
import pandas as pd

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
from telethon import TelegramClient
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è API –∏ –ë–æ—Ç–∞
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from aiogram import Bot, Dispatcher, types, F
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from aiogram.fsm.storage.memory import MemoryStorage

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

# KEYS - TELEGRAM
TG_API_ID = os.getenv('TG_API_ID')
TG_API_HASH = os.getenv('TG_API_HASH')
TARGET_CHANNEL = os.getenv('TARGET_CHANNEL')
TG_BOT_TOKEN = os.getenv('TG_BOT_TOKEN')

# KEYS - VK
VK_ACCESS_TOKEN = os.getenv('VK_ACCESS_TOKEN')
VK_GROUP_DOMAIN = os.getenv('VK_GROUP_DOMAIN')

# KEYS - YOUTUBE & RUTUBE
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')
YOUTUBE_CHANNEL_HANDLE = os.getenv('YOUTUBE_CHANNEL_ID')
RUTUBE_CHANNEL_NAME = os.getenv('RUTUBE_CHANNEL_NAME', 'mts')

# KEYS - HABR
HABR_TARGET_COMPANIES = os.getenv('HABR_TARGET_COMPANIES', 'mts_ai,telegram,vk').split(',')

# KEYS - MWS & AI
MWS_TOKEN = os.getenv('MWS_TOKEN')
MWS_TABLE_ID = os.getenv('MWS_TABLE_ID')
MWS_VIEW_ID = os.getenv('MWS_VIEW_ID')
MWS_CHANNELS_TABLE_ID = os.getenv('MWS_CHANNELS_TABLE_ID')
MWS_CHANNELS_VIEW_ID = os.getenv('MWS_CHANNELS_VIEW_ID')
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
MWS_API_URL = "https://tables.mws.ru/fusion/v1/datasheets"

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ---
app = FastAPI(title="MTS ANALYZER", version="2.0")
app.add_middleware(
    CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

bot = Bot(token=TG_BOT_TOKEN)
storage = MemoryStorage()
dp = Dispatcher(storage=storage)


class ChatRequest(BaseModel):
    question: str


class ChatResponse(BaseModel):
    answer: str


# --- MWS HELPERS ---
class MWSTablesAPI:
    def __init__(self, token, table_id, view_id):
        self.base_url = f"{MWS_API_URL}/{table_id}/records"
        self.headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
        self.view_id = view_id

    def get_existing_links(self):
        try:
            params = {"viewId": self.view_id, "fieldKey": "name", "fields": ["–°—Å—ã–ª–∫–∞"]}
            response = requests.get(self.base_url, headers=self.headers, params=params)
            if response.status_code == 200:
                records = response.json().get('data', {}).get('records', [])
                return {r['fields'].get('–°—Å—ã–ª–∫–∞') for r in records if r['fields'].get('–°—Å—ã–ª–∫–∞')}
            return set()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–µ–π: {e}")
            return set()

    def add_records(self, records_data):
        if not records_data: return
        params = {"viewId": self.view_id, "fieldKey": "name"}
        payload = {"records": [{"fields": rec} for rec in records_data], "fieldKey": "name"}
        try:
            response = requests.post(self.base_url, headers=self.headers, params=params, json=payload)
            response.raise_for_status()
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(records_data)} –∑–∞–ø–∏—Å–µ–π –≤ MWS")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ MWS: {e}")


def get_mws_data():
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    try:
        url = f"{MWS_API_URL}/{MWS_TABLE_ID}/records"
        headers = {"Authorization": f"Bearer {MWS_TOKEN}", "Content-Type": "application/json"}
        # –ë–µ—Ä–µ–º 1000 –∑–∞–ø–∏—Å–µ–π (–º–∞–∫—Å–∏–º—É–º API)
        params = {"viewId": MWS_VIEW_ID, "fieldKey": "name", "pageSize": 1000}
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            return response.json().get('data', {}).get('records', [])
        return []
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è MWS: {e}")
        return []


# --- AI HELPERS ---
def analyze_text_with_llm(text):
    if not OPENROUTER_API_KEY or len(text) < 5: return "Neutral", "–ê–≤—Ç–æ-—Å–∞–º–º–∞—Ä–∏"
    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
    prompt = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç. 1. –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (Positive/Negative/Neutral). 2. –°–∞–º–º–∞—Ä–∏ (1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ).\n–¢–µ–∫—Å—Ç: {text[:800]}\n–í–µ—Ä–Ω–∏ JSON: {{\"sentiment\": \"...\", \"summary\": \"...\"}}"
    try:
        data = {"model": "meta-llama/llama-3.3-70b-instruct:free", "messages": [{"role": "user", "content": prompt}]}
        res = requests.post(url, headers=headers, json=data, timeout=10)
        if res.status_code == 200:
            content = res.json()['choices'][0]['message']['content']
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(0))
                return parsed.get("sentiment", "Neutral"), parsed.get("summary", "")
    except Exception:
        pass
    return "Neutral", "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"


def get_smart_answer(question: str) -> str:
    try:
        records = get_mws_data()
        if not records:
            return "–£ –º–µ–Ω—è –ø–æ–∫–∞ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

        # --- –®–ê–ì 1: –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –Ω–∞ Python (–¢–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞) ---
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∏—â–µ–º –ª–∏–¥–µ—Ä–æ–≤ Python-–æ–º, —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ LLM –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ

        # –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ –ª–∞–π–∫–∞–º
        top_like = max(records, key=lambda x: x.get('fields', {}).get('–õ–∞–π–∫–∏', 0))
        top_like_title = top_like['fields'].get('–ù–∞–∑–≤–∞–Ω–∏–µ', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        max_likes = top_like['fields'].get('–õ–∞–π–∫–∏', 0)

        # –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º
        top_view = max(records, key=lambda x: x.get('fields', {}).get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0))
        top_view_title = top_view['fields'].get('–ù–∞–∑–≤–∞–Ω–∏–µ', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
        max_views = top_view['fields'].get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0)

        # –û–±—â–∞—è —Å—É–º–º–∞
        total_views = sum(r.get('fields', {}).get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0) for r in records)

        # --- –®–ê–ì 2: –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç ---
        # –ú—ã —è–≤–Ω–æ –≥–æ–≤–æ—Ä–∏–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        stats_summary = f"""
        –í–ê–ñ–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê (–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ —Ü–∏—Ñ—Ä—ã –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤):
        - –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤ –≤ –±–∞–∑–µ: {len(records)}
        - –û–±—â–µ–µ —á–∏—Å–ª–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {total_views}
        - –†–ï–ö–û–†–î –ü–û –õ–ê–ô–ö–ê–ú: "{top_like_title}" ({max_likes} –ª–∞–π–∫–æ–≤)
        - –†–ï–ö–û–†–î –ü–û –ü–†–û–°–ú–û–¢–†–ê–ú: "{top_view_title}" ({max_views} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤)
        """

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—É–≤–µ–ª–∏—á–∏–º –¥–æ 20)
        last_posts_context = "–ü–û–°–õ–ï–î–ù–ò–ï –ü–£–ë–õ–ò–ö–ê–¶–ò–ò:\n"
        for r in records[-20:]:
            f = r.get("fields", {})
            title = f.get('–ù–∞–∑–≤–∞–Ω–∏–µ', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]
            last_posts_context += f"- [{f.get('–ò—Å—Ç–æ—á–Ω–∏–∫')}] {title} | –õ–∞–π–∫–æ–≤: {f.get('–õ–∞–π–∫–∏')} | –¢–æ–Ω: {f.get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å')}\n"

        # --- –®–ê–ì 3: –ó–∞–ø—Ä–æ—Å –∫ LLM ---
        prompt = f"""
        –¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.

        {stats_summary}

        {last_posts_context}

        –í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {question}
        """

        ai_url = "https://openrouter.ai/api/v1/chat/completions"
        ai_headers = {
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "HTTP-Referer": "https://github.com/mws-hack",
        }
        ai_data = {
            "model": "meta-llama/llama-3.3-70b-instruct:free",
            "messages": [{"role": "user", "content": prompt}]
        }

        response = requests.post(ai_url, headers=ai_headers, json=ai_data, timeout=30)

        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            logger.error(f"LLM Error: {response.text}")
            return "–ù–µ–π—Ä–æ—Å–µ—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ —è –∑–Ω–∞—é, —á—Ç–æ —Ç–æ–ø –ø–æ –ª–∞–π–∫–∞–º: " + top_like_title

    except Exception as e:
        logger.error(f"Smart Answer Error: {e}")
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}"


# --- SCRAPERS ---
def get_real_channel_id(youtube, input_str):
    if input_str.startswith("UC"): return input_str
    handle = input_str if input_str.startswith("@") else f"@{input_str}"
    try:
        resp = youtube.channels().list(part="id", forHandle=handle).execute()
        if resp["items"]: return resp["items"][0]["id"]
    except Exception:
        pass
    return None


async def fetch_telegram(existing_links, targets):
    """targets: —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ ['durov', 'mts_news']"""
    if not TG_API_ID or not targets: return []

    logger.info(f"üì° TG: –ü–∞—Ä—Å–∏–º –∫–∞–Ω–∞–ª—ã: {targets}")
    client = TelegramClient('anon_session', int(TG_API_ID), TG_API_HASH)
    await client.start()

    new_posts = []

    for channel in targets:
        try:
            async for message in client.iter_messages(channel, limit=5):
                if message.text:
                    link = f"https://t.me/{channel}/{message.id}"
                    if link in existing_links: continue

                    # (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
                    if len(message.text) < 5: continue

                    sentiment, summary = analyze_text_with_llm(message.text)
                    likes = sum(r.count for r in
                                message.reactions.results) if message.reactions and message.reactions.results else 0

                    new_posts.append({
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": message.text[:50].replace('\n', ' ') + "...",
                        "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞": message.text, "–î–∞—Ç–∞": message.date.strftime('%Y-%m-%d'),
                        "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": message.views or 0, "–ò—Å—Ç–æ—á–Ω–∏–∫": "Telegram", "–°—Å—ã–ª–∫–∞": link,
                        "–õ–∞–π–∫–∏": likes, "–†–µ–ø–æ—Å—Ç—ã": getattr(message, 'forwards', 0) or 0,
                        "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": sentiment, "AI –°–∞–º–º–∞—Ä–∏": summary
                    })
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ TG –∫–∞–Ω–∞–ª–∞ {channel}: {e}")

    await client.disconnect()
    return new_posts


def fetch_vk(existing_links, targets):
    """targets: —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ ['mts', 'durov']"""
    if not VK_ACCESS_TOKEN or not targets: return []
    logger.info(f"üîµ VK: –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—ã: {targets}")

    new_posts = []
    try:
        vk_session = vk_api.VkApi(token=VK_ACCESS_TOKEN)
        vk = vk_session.get_api()

        for domain in targets:
            try:
                response = vk.wall.get(domain=domain, count=5)
                for post in response['items']:
                    link = f"https://vk.com/wall{post['owner_id']}_{post['id']}"
                    if link in existing_links: continue

                    text = post.get('text', '')
                    if not text: continue

                    sentiment, summary = analyze_text_with_llm(text)
                    date_str = datetime.fromtimestamp(post['date']).strftime('%Y-%m-%d')

                    new_posts.append({
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": text[:50].replace('\n', ' ') + "...",
                        "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞": text, "–î–∞—Ç–∞": date_str,
                        "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": post.get('views', {}).get('count', 0), "–ò—Å—Ç–æ—á–Ω–∏–∫": "VK", "–°—Å—ã–ª–∫–∞": link,
                        "–õ–∞–π–∫–∏": post.get('likes', {}).get('count', 0),
                        "–†–µ–ø–æ—Å—Ç—ã": post.get('reposts', {}).get('count', 0),
                        "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": sentiment, "AI –°–∞–º–º–∞—Ä–∏": summary
                    })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ VK –¥–æ–º–µ–Ω–∞ {domain}: {e}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ VK: {e}")

    return new_posts


def fetch_youtube(existing_links, targets):
    """targets: —Å–ø–∏—Å–æ–∫ handle ['@mts', '@google']"""
    if not YOUTUBE_API_KEY or not targets: return []
    logger.info(f"üì∫ YT: –ü–∞—Ä—Å–∏–º –∫–∞–Ω–∞–ª—ã: {targets}")

    youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
    new_vids = []

    for handle in targets:
        cid = get_real_channel_id(youtube, handle)
        if not cid: continue

        try:
            req = youtube.search().list(part="snippet", channelId=cid, maxResults=5, order="date", type="video")
            res = req.execute()

            for item in res.get('items', []):
                vid = item['id']['videoId']
                link = f"https://www.youtube.com/watch?v={vid}"
                if link in existing_links: continue

                snippet = item['snippet']
                stats = youtube.videos().list(part="statistics", id=vid).execute()['items'][0]['statistics']
                sentiment, summary = analyze_text_with_llm(snippet['title'])

                new_vids.append({
                    "–ù–∞–∑–≤–∞–Ω–∏–µ": snippet['title'], "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞": snippet['description'],
                    "–î–∞—Ç–∞": snippet['publishedAt'][:10], "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": int(stats.get('viewCount', 0)),
                    "–ò—Å—Ç–æ—á–Ω–∏–∫": "YouTube", "–°—Å—ã–ª–∫–∞": link, "–õ–∞–π–∫–∏": int(stats.get('likeCount', 0)),
                    "–†–µ–ø–æ—Å—Ç—ã": int(stats.get('commentCount', 0)), "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": sentiment, "AI –°–∞–º–º–∞—Ä–∏": summary
                })
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ YT –∫–∞–Ω–∞–ª–∞ {handle}: {e}")

    return new_vids


def fetch_rutube_data(existing_links, targets):
    """targets: —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤"""
    if not targets: return []
    logger.info(f"üî¥ Rutube: –ü–∞—Ä—Å–∏–º –∫–∞–Ω–∞–ª—ã: {targets}")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    rutube_posts = []
    for identifier in targets:
        try:
            # 1. –û—á–∏—Å—Ç–∫–∞ ID
            if "rutube.ru" in identifier:
                if "/channel/" in identifier:
                    identifier = identifier.split("/channel/")[1].split("/")[0]
                elif "/u/" in identifier:
                    identifier = identifier.split("/u/")[1].split("/")[0]
            identifier = identifier.strip()

            if not identifier: continue

            # 2. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –≤–∏–¥–µ–æ –∫–∞–Ω–∞–ª–∞

            videos_url = f"https://rutube.ru/api/video/person/{identifier}/"

            response = requests.get(videos_url, headers=headers, timeout=10)

            if response.status_code == 404:
                logger.warning(f"‚ö†Ô∏è Rutube: –ö–∞–Ω–∞–ª {identifier} –Ω–µ –Ω–∞–π–¥–µ–Ω (404). –ü—Ä–æ–≤–µ—Ä—å ID.")
                continue
            if response.status_code != 200:
                logger.error(f"‚ö†Ô∏è Rutube API Error: {response.status_code}")
                continue

            data = response.json()
            results = data.get('results', [])

            for video in results[:5]:

                video_uuid = video.get('id')
                link = f"https://rutube.ru/video/{video_uuid}/"

                if link in existing_links: continue

                desc = video.get('description', '') or video.get('title', '')
                sentiment, summary = analyze_text_with_llm(desc)

                rutube_posts.append({
                    "–ù–∞–∑–≤–∞–Ω–∏–µ": video.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50] + "...",
                    "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞": desc,
                    "–î–∞—Ç–∞": video.get('created_ts', '').split('T')[0],
                    "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": video.get('hits', 0),  # hits = –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
                    "–õ–∞–π–∫–∏": 0,  # –í –æ–±—â–µ–π –ª–µ–Ω—Ç–µ –ª–∞–π–∫–∏ –Ω–µ –æ—Ç–¥–∞—é—Ç—Å—è
                    "–†–µ–ø–æ—Å—Ç—ã": 0,
                    "–ò—Å—Ç–æ—á–Ω–∏–∫": "Rutube",
                    "–°—Å—ã–ª–∫–∞": link,
                    "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": sentiment,
                    "AI –°–∞–º–º–∞—Ä–∏": summary
                })
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Rutube {identifier}: {e}")

    return rutube_posts


def parse_habr_metric(value_str):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ '1.5k', '+10', '120' –≤ —á–∏—Å–ª–∞"""
    if not value_str:
        return 0
    try:
        value_str = value_str.strip().replace('+', '').replace(',', '.')
        if 'k' in value_str.lower():
            return int(float(value_str.lower().replace('k', '')) * 1000)
        if 'm' in value_str.lower():  # –ú–∏–ª–ª–∏–æ–Ω—ã (—Ä–µ–¥–∫–æ, –Ω–æ –±—ã–≤–∞–µ—Ç)
            return int(float(value_str.lower().replace('m', '')) * 1000000)
        return int(float(value_str))
    except ValueError:
        return 0


def parse_habr_post(post_url):
    """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ –Ω–∞ Habr"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7"
        }

        response = requests.get(post_url, headers=headers, timeout=10)
        if response.status_code != 200:
            logger.warning(f"Habr post error {response.status_code}: {post_url}")
            return None

        soup = BeautifulSoup(response.content, 'html.parser')

        # 1. –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title_elem = soup.find('h1', class_='tm-title')
        title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

        # 2. –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ—Å—Ç–∞ (–•–∞–±—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç id="post-content-body")
        content_elem = soup.find(id='post-content-body')
        if not content_elem:
            # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ –∫–ª–∞—Å—Å—É
            content_elem = soup.find('div', class_='tm-article-body')

        # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç, —Ä–∞–∑–¥–µ–ª—è—è –∞–±–∑–∞—Ü—ã –ø—Ä–æ–±–µ–ª–∞–º–∏
        content = content_elem.get_text(separator=' ', strip=True) if content_elem else ""

        # 3. –î–∞—Ç–∞ (ISO —Ñ–æ—Ä–º–∞—Ç –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–∞ time)
        date_elem = soup.find('time')
        date = date_elem.get('datetime', '')[:10] if date_elem else datetime.now().strftime('%Y-%m-%d')

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        views = 0
        likes = 0
        comments = 0

        # –†–µ–π—Ç–∏–Ω–≥ (–õ–∞–π–∫–∏) - –∏—â–µ–º —Å—á–µ—Ç—á–∏–∫ —Ä–µ–π—Ç–∏–Ω–≥–∞
        # –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ .tm-votes-meter__value
        likes_elem = soup.find('span', class_='tm-votes-meter__value')
        if likes_elem:
            likes = parse_habr_metric(likes_elem.get_text())

        # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã - –∏—â–µ–º –∏–∫–æ–Ω–∫—É –≥–ª–∞–∑–∞ –∏ —Å–æ—Å–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç
        # –û–±—ã—á–Ω–æ —ç—Ç–æ –∫–ª–∞—Å—Å tm-icon-counter__value
        # –ù–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å—Ç–∞—Ç—å–∏ –±–ª–æ–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥—Ä—É–≥–∏–º
        stats_blocks = soup.find_all('span', class_='tm-icon-counter__value')
        if stats_blocks:
            # –û–±—ã—á–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∏–ª–∏ –≤—Ç–æ—Ä–æ–π —Å—á–µ—Ç—á–∏–∫ —Å –±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã (–æ–±—ã—á–Ω–æ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ 'views')
            # –ß–∞—Å—Ç–æ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∏–¥—É—Ç –ø–æ—Å–ª–µ —Ä–µ–π—Ç–∏–Ω–≥–∞.
            for stat in stats_blocks:
                val = parse_habr_metric(stat.get_text())
                if val > views:  # –ë–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ, –æ–±—ã—á–Ω–æ —ç—Ç–æ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
                    views = val

        # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        comments_elem = soup.find('span', class_='tm-article-comments-counter-link__value')
        if comments_elem:
            comments = parse_habr_metric(comments_elem.get_text())

        return {
            'title': title,
            'content': content,
            'date': date,
            'views': views,
            'likes': likes,
            'comments': comments,
            'shares': 0
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ—Å—Ç–∞ Habr {post_url}: {e}")
        return None


def fetch_habr_data(existing_links, targets):
    """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ—Å—Ç–æ–≤ —Å –•–∞–±—Ä–∞ –ø–æ —Å–ø–∏—Å–∫—É –∫–æ–º–ø–∞–Ω–∏–π"""
    if not targets:
        return []

    logger.info(f"üìù Habr: –ü–∞—Ä—Å–∏–º –∫–æ–º–ø–∞–Ω–∏–∏: {targets}")
    habr_posts = []

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    for company in targets:
        try:
            # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Ç URL –µ—Å–ª–∏ —Å–ª—É—á–∞–π–Ω–æ –ø–æ–ø–∞–ª
            company = company.strip()
            if "habr.com" in company:
                company = company.split('/companies/')[-1].replace('/articles/', '').replace('/', '')

            search_url = f"https://habr.com/ru/companies/{company}/articles/"

            response = requests.get(search_url, headers=headers, timeout=10)
            if response.status_code != 200:
                logger.warning(f"Habr: –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è {company} (Code: {response.status_code})")
                continue

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –≤ —Å–ø–∏—Å–∫–µ
            articles = soup.find_all('article', class_='tm-articles-list__item')

            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5
            for post in articles[:5]:
                try:
                    title_elem = post.find('h2', class_='tm-title')
                    if not title_elem: continue

                    link_elem = title_elem.find('a')
                    if not link_elem: continue

                    relative_link = link_elem.get('href', '')
                    full_link = f"https://habr.com{relative_link}"

                    if full_link in existing_links: continue

                    logger.info(f"Habr: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {full_link}")

                    # –ü—Ä–æ–≤–∞–ª–∏–≤–∞–µ–º—Å—è –≤–Ω—É—Ç—Ä—å —Å—Ç–∞—Ç—å–∏ –∑–∞ –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    post_data = parse_habr_post(full_link)

                    if not post_data:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –ø–æ—Å—Ç–∞ {full_link}")
                        continue

                    # –ê–Ω–∞–ª–∏–∑ AI (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1500 —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç)
                    sentiment, summary = analyze_text_with_llm(post_data['content'][:1500])

                    habr_posts.append({
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": post_data['title'][:100],  # MWS –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –ª–∏–º–∏—Ç –Ω–∞ –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞": post_data['content'][:2000] + "...",  # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                        "–î–∞—Ç–∞": post_data['date'],
                        "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã": post_data['views'],
                        "–õ–∞–π–∫–∏": post_data['likes'],
                        "–†–µ–ø–æ—Å—Ç—ã": post_data['shares'],  # –•–∞–±—Ä –Ω–µ –æ—Ç–¥–∞–µ—Ç —à–µ—Ä—ã –≤ –ø–∞–±–ª–∏–∫
                        "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏": post_data['comments'],
                        "–ò—Å—Ç–æ—á–Ω–∏–∫": "Habr",
                        "–°—Å—ã–ª–∫–∞": full_link,
                        "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": sentiment,
                        "AI –°–∞–º–º–∞—Ä–∏": summary
                    })

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–ø–∏—Å–∫–∞ Habr: {e}")
                    continue

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Habr –∫–æ–º–ø–∞–Ω–∏–∏ {company}: {e}")
            continue

    return habr_posts


async def update_data_logic():
    mws = MWSTablesAPI(MWS_TOKEN, MWS_TABLE_ID, MWS_VIEW_ID)

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ä—ã—Ö –ø–æ—Å—Ç–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å)
    existing = mws.get_existing_links()

    # 2. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –ò–ó –¢–ê–ë–õ–ò–¶–´ MWS
    channels = get_monitored_channels()

    if not channels:
        logger.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å.")
        return

    logger.info(f"üìã –ù–∞–π–¥–µ–Ω—ã –∫–∞–Ω–∞–ª—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {channels}")

    # 3. –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–µ—Ä—ã —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ —Å–ø–∏—Å–∫–∞–º–∏
    data_tg = await fetch_telegram(existing, channels.get('Telegram', []))
    data_yt = fetch_youtube(existing, channels.get('YouTube', []))
    data_ru = fetch_rutube_data(existing, channels.get('Rutube', []))
    data_vk = fetch_vk(existing, channels.get('VK', []))
    data_habr = fetch_habr_data(existing, channels.get('Habr', []))

    logger.info(
        f"üìä –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: TG={len(data_tg)}, YT={len(data_yt)}, RU={len(data_ru)}, VK={len(data_vk)}, Habr={len(data_habr)}")

    all_data = data_tg + data_yt + data_ru + data_vk + data_habr

    if all_data:
        mws.add_records(all_data)
        logger.info(f"üéâ –£—Å–ø–µ—Ö! –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤.")
    else:
        logger.info("üò¥ –°–≤–µ–∂–∏—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")


def get_monitored_channels():
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ MWS –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏—Ö –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: {'Telegram': ['durov', ...], 'VK': ['mts', ...], ...}
    """
    try:
        url = f"{MWS_API_URL}/{MWS_CHANNELS_TABLE_ID}/records"
        headers = {"Authorization": f"Bearer {MWS_TOKEN}", "Content-Type": "application/json"}
        params = {"viewId": MWS_CHANNELS_VIEW_ID, "fieldKey": "name", "pageSize": 1000}

        response = requests.get(url, headers=headers, params=params)
        if response.status_code != 200:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –∫–∞–Ω–∞–ª–æ–≤: {response.status_code}")
            return {}

        records = response.json().get('data', {}).get('records', [])
        channels = {"Telegram": [], "VK": [], "YouTube": [], "Rutube": [], "Habr": []}

        for r in records:
            fields = r.get('fields', {})

            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–º–æ—Ç—Ä–µ—Ç—å —ç—Ç–æ—Ç –∫–∞–Ω–∞–ª
            if fields.get('–¢–∏–ø –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏') != '–°–º–æ—Ç—Ä–µ—Ç—å':
                continue

            source = fields.get('–ò—Å—Ç–æ—á–Ω–∏–∫')
            raw_link = fields.get('–ò–º—è –∫–∞–Ω–∞–ª–∞', '').strip()

            if not source or not raw_link:
                continue

            # 2. –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫—É –¥–æ ID/Handle
            clean_id = raw_link

            if source == "Telegram":
                clean_id = raw_link.replace("https://t.me/", "").replace("@", "")
            elif source == "VK":
                clean_id = raw_link.replace("https://vk.com/", "").replace("https://m.vk.com/", "")
            elif source == "YouTube":
                clean_id = raw_link.replace("https://www.youtube.com/", "").replace("https://youtube.com/", "")
            elif source == "Rutube":
                clean_id = raw_link.replace("https://rutube.ru/channel/", "").replace("/", "")
            elif source == "Habr":
                # –î–ª—è Habr –±–µ—Ä–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∫–∞–∫ –µ—Å—Ç—å (–±–µ–∑ URL)
                if "habr.com" in raw_link:
                    clean_id = raw_link.replace("https://habr.com/ru/company/", "").replace("/", "")
                else:
                    clean_id = raw_link  # –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫, —É–¥–∞–ª—è—è –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if source in channels and clean_id:
                channels[source].append(clean_id)

        # –ï—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ Habr, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ .env
        if not channels["Habr"] and HABR_TARGET_COMPANIES:
            channels["Habr"] = [company.strip() for company in HABR_TARGET_COMPANIES if company.strip()]

        logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–∞–Ω–∞–ª—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {channels}")
        return channels

    except Exception as e:
        logger.error(f"Critical error getting channels: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–Ω–∞–ª—ã –∏–∑ .env –µ—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
        return {"Telegram": [], "VK": [], "YouTube": [], "Rutube": [], "Habr": HABR_TARGET_COMPANIES}


# === FRONTEND ANALYTICS ENDPOINTS ===

@app.get("/api/info", summary="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ")
async def get_system_info():
    """
    –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö
    """
    return {
        "project": "MTS Content Analyzer",
        "description": "–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π —Å AI-–∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π",
        "version": "1.0",
        "features": [
            "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–æ—Ü—Å–µ—Ç–µ–π",
            "AI-–∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–º–º–∞—Ä–∏ —á–µ—Ä–µ–∑ LLM",
            "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
        ],
        "data_sources": [
            {
                "name": "Telegram",
                "status": "active",
                "collected_data": [
                    "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏", "–†–µ–ø–æ—Å—Ç—ã",
                    "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "AI-—Å–∞–º–º–∞—Ä–∏"
                ]
            },
            {
                "name": "VK",
                "status": "planned",
                "collected_data": [
                    "–ü–æ—Å—Ç—ã", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏",
                    "–†–µ–ø–æ—Å—Ç—ã", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "AI-–∞–Ω–∞–ª–∏–∑"
                ]
            },
            {
                "name": "Rutube",
                "status": "active",
                "collected_data": [
                    "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏",
                    "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "AI-—Å–∞–º–º–∞—Ä–∏"
                ]
            },
            {
                "name": "Habr",
                "status": "active",
                "collected_data": [
                    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞—Ç—å–∏", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏",
                    "–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "AI-–∞–Ω–∞–ª–∏–∑"
                ]
            }
        ],
        "ai_capabilities": [
            "–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (Positive/Negative/Neutral)",
            "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–Ω—Ç–µ–Ω—Ç–µ",
            "–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π"
        ],
        "total_records": len(get_mws_data())
    }


@app.get("/api/data", summary="–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ")
async def get_all_data(
        limit: int = Query(100, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π"),
        offset: int = Query(0, description="–°–º–µ—â–µ–Ω–∏–µ"),
        source: Optional[str] = Query(None, description="–§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É"),
        sentiment: Optional[str] = Query(None, description="–§–∏–ª—å—Ç—Ä –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")
):
    """
    –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    """
    try:
        all_data = get_mws_data()

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        filtered_data = all_data
        if source:
            filtered_data = [r for r in filtered_data if r.get('fields', {}).get('–ò—Å—Ç–æ—á–Ω–∏–∫') == source]
        if sentiment:
            filtered_data = [r for r in filtered_data if r.get('fields', {}).get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å') == sentiment]

        # –ü–∞–≥–∏–Ω–∞—Ü–∏—è
        total = len(filtered_data)
        paginated_data = filtered_data[offset:offset + limit]

        return {
            "total": total,
            "limit": limit,
            "offset": offset,
            "filters": {
                "source": source,
                "sentiment": sentiment
            },
            "data": [
                {
                    "id": f"{record.get('fields', {}).get('–ò—Å—Ç–æ—á–Ω–∏–∫', 'unknown')}_{idx + offset}",
                    "fields": record.get('fields', {}),
                    "metadata": {
                        "text_length": len(record.get('fields', {}).get('–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞', '')),
                        "has_ai_summary": bool(record.get('fields', {}).get('AI –°–∞–º–º–∞—Ä–∏'))
                    }
                }
                for idx, record in enumerate(paginated_data)
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {str(e)}")


@app.get("/api/stats/overview", summary="–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
async def get_overview_stats():
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º
    """
    data = get_mws_data()

    if not data:
        return {"message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}

    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_posts = len(data)
    sources = {}
    sentiments = {"Positive": 0, "Negative": 0, "Neutral": 0}

    total_views = 0
    total_likes = 0
    total_comments = 0

    for record in data:
        fields = record.get('fields', {})
        source = fields.get('–ò—Å—Ç–æ—á–Ω–∏–∫', 'Unknown')
        sentiment = fields.get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', 'Neutral')

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        if source not in sources:
            sources[source] = {"count": 0, "views": 0, "likes": 0}
        sources[source]["count"] += 1
        sources[source]["views"] += fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0)
        sources[source]["likes"] += fields.get('–õ–∞–π–∫–∏', 0)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if sentiment in sentiments:
            sentiments[sentiment] += 1

        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        total_views += fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0)
        total_likes += fields.get('–õ–∞–π–∫–∏', 0)
        total_comments += fields.get('–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏', 0)

    # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    avg_views = total_views / total_posts if total_posts > 0 else 0
    avg_likes = total_likes / total_posts if total_posts > 0 else 0
    engagement_rate = (total_likes / total_views * 100) if total_views > 0 else 0

    return {
        "summary": {
            "total_posts": total_posts,
            "total_views": total_views,
            "total_likes": total_likes,
            "total_comments": total_comments,
            "average_views": round(avg_views, 2),
            "average_likes": round(avg_likes, 2),
            "engagement_rate": round(engagement_rate, 2)
        },
        "sources": sources,
        "sentiments": sentiments,
        "content_effectiveness": {
            "most_engaging_source": max(sources.items(), key=lambda x: x[1]["likes"])[0] if sources else "N/A",
            "positive_content_ratio": round(sentiments["Positive"] / total_posts * 100, 2) if total_posts > 0 else 0,
            "top_performing_metric": "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã" if total_views > total_likes else "–õ–∞–π–∫–∏"
        }
    }


@app.get("/api/analytics/sentiment", summary="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")
async def get_sentiment_analytics():
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    data = get_mws_data()

    sentiment_stats = {"Positive": 0, "Negative": 0, "Neutral": 0}
    source_sentiment = {}
    sentiment_engagement = {"Positive": 0, "Negative": 0, "Neutral": 0}

    for record in data:
        fields = record.get('fields', {})
        sentiment = fields.get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', 'Neutral')
        source = fields.get('–ò—Å—Ç–æ—á–Ω–∏–∫', 'Unknown')
        views = fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0)
        likes = fields.get('–õ–∞–π–∫–∏', 0)

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if sentiment in sentiment_stats:
            sentiment_stats[sentiment] += 1

        # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        if source not in source_sentiment:
            source_sentiment[source] = {"Positive": 0, "Negative": 0, "Neutral": 0, "total": 0}
        if sentiment in source_sentiment[source]:
            source_sentiment[source][sentiment] += 1
            source_sentiment[source]["total"] += 1

        # –í–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if sentiment in sentiment_engagement:
            sentiment_engagement[sentiment] += views + likes

    # –†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
    total_posts = len(data)
    sentiment_percentages = {
        sentiment: round((count / total_posts) * 100, 2)
        for sentiment, count in sentiment_stats.items()
    } if total_posts > 0 else {}

    return {
        "overall": {
            "counts": sentiment_stats,
            "percentages": sentiment_percentages,
            "dominant_sentiment": max(sentiment_stats.items(), key=lambda x: x[1])[0] if sentiment_stats else "Neutral"
        },
        "by_source": source_sentiment,
        "engagement_by_sentiment": sentiment_engagement,
        "insights": {
            "total_analyzed": total_posts,
            "most_positive_source": max(source_sentiment.items(), key=lambda x: x[1]["Positive"])[
                0] if source_sentiment else "N/A",
            "engagement_trend": "Positive" if sentiment_engagement["Positive"] > sentiment_engagement[
                "Negative"] else "Neutral"
        }
    }


@app.get("/api/top/content", summary="–¢–æ–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
async def get_top_content(
        metric: str = Query("–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", description="–ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏"),
        limit: int = Query(10, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π"),
        source: Optional[str] = Query(None, description="–§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É")
):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ
    """
    valid_metrics = ["–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏", "–†–µ–ø–æ—Å—Ç—ã", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏"]
    if metric not in valid_metrics:
        raise HTTPException(
            status_code=400,
            detail=f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–∞—è –º–µ—Ç—Ä–∏–∫–∞. –î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {', '.join(valid_metrics)}"
        )

    data = get_mws_data()

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
    if source:
        data = [r for r in data if r.get('fields', {}).get('–ò—Å—Ç–æ—á–Ω–∏–∫') == source]

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ
    sorted_data = sorted(
        data,
        key=lambda x: x.get('fields', {}).get(metric, 0),
        reverse=True
    )[:limit]

    return {
        "metric": metric,
        "source_filter": source,
        "top_content": [
            {
                "rank": idx + 1,
                "title": record.get('fields', {}).get('–ù–∞–∑–≤–∞–Ω–∏–µ', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'),
                "source": record.get('fields', {}).get('–ò—Å—Ç–æ—á–Ω–∏–∫', 'Unknown'),
                "metric_value": record.get('fields', {}).get(metric, 0),
                "sentiment": record.get('fields', {}).get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', 'Neutral'),
                "date": record.get('fields', {}).get('–î–∞—Ç–∞', ''),
                "link": record.get('fields', {}).get('–°—Å—ã–ª–∫–∞', ''),
                "ai_summary": record.get('fields', {}).get('AI –°–∞–º–º–∞—Ä–∏', '')
            }
            for idx, record in enumerate(sorted_data)
        ]
    }


@app.get("/api/sources/performance", summary="–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
async def get_sources_performance():
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    data = get_mws_data()

    sources = {}

    for record in data:
        fields = record.get('fields', {})
        source = fields.get('–ò—Å—Ç–æ—á–Ω–∏–∫', 'Unknown')

        if source not in sources:
            sources[source] = {
                "posts_count": 0,
                "total_views": 0,
                "total_likes": 0,
                "total_comments": 0,
                "sentiments": {"Positive": 0, "Negative": 0, "Neutral": 0},
                "posts": []
            }

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        sources[source]["posts_count"] += 1
        sources[source]["total_views"] += fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0)
        sources[source]["total_likes"] += fields.get('–õ–∞–π–∫–∏', 0)
        sources[source]["total_comments"] += fields.get('–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏', 0)

        # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiment = fields.get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', 'Neutral')
        if sentiment in sources[source]["sentiments"]:
            sources[source]["sentiments"][sentiment] += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å—Ç –¥–ª—è –¥–µ—Ç–∞–ª–µ–π
        sources[source]["posts"].append({
            "title": fields.get('–ù–∞–∑–≤–∞–Ω–∏–µ', ''),
            "views": fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0),
            "likes": fields.get('–õ–∞–π–∫–∏', 0),
            "sentiment": sentiment,
            "date": fields.get('–î–∞—Ç–∞', '')
        })

    # –†–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    for source, stats in sources.items():
        stats["average_views"] = round(stats["total_views"] / stats["posts_count"], 2) if stats[
                                                                                              "posts_count"] > 0 else 0
        stats["average_likes"] = round(stats["total_likes"] / stats["posts_count"], 2) if stats[
                                                                                              "posts_count"] > 0 else 0
        stats["engagement_rate"] = round((stats["total_likes"] / stats["total_views"] * 100), 2) if stats[
                                                                                                        "total_views"] > 0 else 0
        stats["positive_ratio"] = round((stats["sentiments"]["Positive"] / stats["posts_count"] * 100), 2) if stats[
                                                                                                                  "posts_count"] > 0 else 0

    return {
        "sources": sources,
        "comparison": {
            "best_engagement": max(sources.items(), key=lambda x: x[1]["engagement_rate"])[0] if sources else "N/A",
            "most_active": max(sources.items(), key=lambda x: x[1]["posts_count"])[0] if sources else "N/A",
            "most_positive": max(sources.items(), key=lambda x: x[1]["positive_ratio"])[0] if sources else "N/A"
        }
    }


@app.get("/api/health", summary="–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –∏ –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        data = get_mws_data()
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "data_available": len(data) > 0,
            "total_records": len(data),
            "sources_available": list(set(
                record.get('fields', {}).get('–ò—Å—Ç–æ—á–Ω–∏–∫', 'Unknown')
                for record in data
            ))
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }


# --- –≠–ö–°–ü–û–†–¢ CSV
# --- –≠–ö–°–ü–û–†–¢ CSV
@app.get("/api/export/csv")
async def export_csv(
        source: Optional[str] = Query(None, description="–§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É"),
        sentiment: Optional[str] = Query(None, description="–§–∏–ª—å—Ç—Ä –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")
):
    try:
        data = get_mws_data()
        if source:
            data = [r for r in data if r.get('fields', {}).get('–ò—Å—Ç–æ—á–Ω–∏–∫') == source]
        if sentiment:
            data = [r for r in data if r.get('fields', {}).get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å') == sentiment]
        if not data:
            raise HTTPException(status_code=404, detail='–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞')

        output = io.StringIO()
        writer = csv.writer(output)
        headers = [
            "–ù–∞–∑–≤–∞–Ω–∏–µ", "–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞", "–î–∞—Ç–∞", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏",
            "–†–µ–ø–æ—Å—Ç—ã", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏", "–ò—Å—Ç–æ—á–Ω–∏–∫", "–°—Å—ã–ª–∫–∞", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å", "AI –°–∞–º–º–∞—Ä–∏"
        ]
        writer.writerow(headers)

        for record in data:
            fields = record.get('fields', {})
            writer.writerow([
                fields.get('–ù–∞–∑–≤–∞–Ω–∏–µ', ''),
                fields.get('–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞', ''),
                fields.get('–î–∞—Ç–∞', ''),
                fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0),
                fields.get('–õ–∞–π–∫–∏', 0),
                fields.get('–†–µ–ø–æ—Å—Ç—ã', 0),
                fields.get('–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏', 0),
                fields.get('–ò—Å—Ç–æ—á–Ω–∏–∫', ''),
                fields.get('–°—Å—ã–ª–∫–∞', ''),
                fields.get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', ''),
                fields.get('AI –°–∞–º–º–∞—Ä–∏', '')
            ])

        # –°–æ–∑–¥–∞–µ–º StreamingResponse –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ü–∏–∫–ª–∞
        response = StreamingResponse(
            iter([output.getvalue()]),
            media_type="text/csv",
            headers={
                "Content-Disposition": f"attachment; filename=content_analysis_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
            }
        )
        return response

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f'–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(e)}')


@dp.message(F.text == "/start")
async def start_menu(message: types.Message):
    buttons = [
        [KeyboardButton(text="üìä –¢–æ–ø –ø–æ—Å—Ç–æ–≤"), KeyboardButton(text="üîÆ –ü—Ä–æ–≥–Ω–æ–∑")],
        [KeyboardButton(text="üì• –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV")]
    ]

    keyboard = ReplyKeyboardMarkup(
        keyboard=buttons,
        resize_keyboard=True,
        input_field_placeholder="üéØ –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–ª–∏ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å..."
    )

    welcome_text = """
‚ú® <b>–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ MWS Content Analyzer!</b> ‚ú®

ü§ñ <i>–í–∞—à —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞</i>

üéØ <b>–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:</b>
‚Ä¢ üìä <b>–¢–æ–ø –ø–æ—Å—Ç–æ–≤</b> - —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
‚Ä¢ üîÆ <b>–ü—Ä–æ–≥–Ω–æ–∑</b> - AI-–∞–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏  
‚Ä¢ üì• <b>–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö</b> - –≤—ã–≥—Ä—É–∑–∫–∞ –≤ CSV —Ñ–æ—Ä–º–∞—Ç–µ

üí¨ <b>–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –≤ —á–∞—Ç:</b>
<i>"–ö–∞–∫–∏–µ –ø–æ—Å—Ç—ã –ø–æ–ª—É—á–∏–ª–∏ –±–æ–ª—å—à–µ –ª–∞–π–∫–æ–≤?"
"–ö–∞–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π?"
"–ü–æ–∫–∞–∂–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ –Ω–µ–¥–µ–ª—é"</i>

üëá <b>–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∏–∂–µ</b>
    """

    await message.answer(
        welcome_text,
        reply_markup=keyboard,
        parse_mode="HTML"
    )


@dp.message(F.text == "üì• –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV")
async def export(message: types.Message):
    reply_markup = ReplyKeyboardMarkup(
        keyboard=[
            [KeyboardButton(text='üîô –í–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥')]
        ],
        resize_keyboard=True
    )
    await message.answer("üîÑ –ì–æ—Ç–æ–≤–ª—é —Ñ–∞–π–ª... ", reply_markup=reply_markup)
    try:
        data = get_mws_data()
        if not data:
            await message.answer("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ ")
            return

        output = io.StringIO()
        writer = csv.writer(output)
        headers = ["–ù–∞–∑–≤–∞–Ω–∏–µ", "–î–∞—Ç–∞", "–ü—Ä–æ—Å–º–æ—Ç—Ä—ã", "–õ–∞–π–∫–∏", "–ò—Å—Ç–æ—á–Ω–∏–∫", "–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"]
        writer.writerow(headers)
        for record in data:
            fields = record.get('fields', {})
            writer.writerow([
                fields.get('–ù–∞–∑–≤–∞–Ω–∏–µ', '')[:100],  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                fields.get('–î–∞—Ç–∞', ''),
                fields.get('–ü—Ä–æ—Å–º–æ—Ç—Ä—ã', 0),
                fields.get('–õ–∞–π–∫–∏', 0),
                fields.get('–ò—Å—Ç–æ—á–Ω–∏–∫', ''),
                fields.get('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', '')
            ])
        csv_data = output.getvalue().encode('utf-8')
        await message.answer_document(
            types.BufferedInputFile(csv_data, filename=f"content_export_{datetime.now().strftime('%Y%m%d')}.csv"),
            caption="‚úÖ –í–∞—à CSV —Ñ–∞–π–ª –≥–æ—Ç–æ–≤!"
        )
    except Exception as e:
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {str(e)}")


@dp.message(F.text == "üîô –í–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞–∑–∞–¥")
async def back_to_main(message: types.Message):
    # –í–µ—Ä–Ω—É—Ç—å—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é
    buttons = [
        [KeyboardButton(text="üìä –¢–æ–ø –ø–æ—Å—Ç–æ–≤"), KeyboardButton(text="üîÆ –ü—Ä–æ–≥–Ω–æ–∑")],
        [KeyboardButton(text="üì• –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV")]
    ]
    keyboard = ReplyKeyboardMarkup(keyboard=buttons, resize_keyboard=True)
    await message.answer("–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é:", reply_markup=keyboard)


@dp.message(F.text)
async def handle_bot_question(message: types.Message):
    await message.bot.send_chat_action(chat_id=message.chat.id, action="typing")
    answer = get_smart_answer(message.text)
    await message.answer(answer)


# --- STARTUP ---
@app.post("/chat", response_model=ChatResponse)
async def chat_api(request: ChatRequest):
    return ChatResponse(answer=get_smart_answer(request.question))


@app.on_event("startup")
async def on_startup():
    asyncio.create_task(update_data_logic())
    asyncio.create_task(dp.start_polling(bot))
    logger.info("üöÄ SYSTEM ONLINE: API + BOT + SCRAPERS")


if __name__ == '__main__':
    uvicorn.run(app, host="0.0.0.0", port=8000)